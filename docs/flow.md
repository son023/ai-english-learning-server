# AI English Learning Server - Lu·ªìng Ho·∫°t ƒê·ªông Chi Ti·∫øt

## T·ªïng Quan H·ªá Th·ªëng

H·ªá th·ªëng AI English Learning Server l√† m·ªôt n·ªÅn t·∫£ng ƒë√°nh gi√° ph√°t √¢m ti·∫øng Anh s·ª≠ d·ª•ng tr√≠ tu·ªá nh√¢n t·∫°o, ƒë∆∞·ª£c x√¢y d·ª±ng v·ªõi FastAPI v√† t√≠ch h·ª£p nhi·ªÅu service AI ti√™n ti·∫øn.

### Ki·∫øn Tr√∫c T·ªïng Th·ªÉ
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client App    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   FastAPI        ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Services      ‚îÇ
‚îÇ   (Frontend)    ‚îÇ    ‚îÇ   (main.py)      ‚îÇ    ‚îÇ   (Microservices‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ                         ‚îÇ
                              ‚ñº                         ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ   Models     ‚îÇ        ‚îÇ ‚Ä¢ WhisperService‚îÇ
                       ‚îÇ (Data Types) ‚îÇ        ‚îÇ ‚Ä¢ AudioService  ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ ‚Ä¢ PronuncService‚îÇ
                                              ‚îÇ ‚Ä¢ LLMService    ‚îÇ
                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 1. MAIN.PY - API Gateway & Orchestrator

### Ch·ª©c NƒÉng Ch√≠nh
`main.py` ƒë√≥ng vai tr√≤ l√† **API Gateway** v√† **Service Orchestrator**, qu·∫£n l√Ω t·∫•t c·∫£ c√°c endpoints v√† ƒëi·ªÅu ph·ªëi lu·ªìng x·ª≠ l√Ω gi·ªØa c√°c services.

### Chi Ti·∫øt C√°c Endpoints

#### 1.1. `GET /` - Root Endpoint
```python
@app.get("/")
async def root():
```
- **Ch·ª©c nƒÉng**: Endpoint ch√†o m·ª´ng c∆° b·∫£n
- **Response**: Th√¥ng ƒëi·ªáp ch√†o m·ª´ng JSON

#### 1.2. `GET /health` - Health Check
```python
@app.get("/health")
async def health_check():
```
- **Ch·ª©c nƒÉng**: Ki·ªÉm tra tr·∫°ng th√°i ho·∫°t ƒë·ªông c·ªßa to√†n b·ªô h·ªá th·ªëng
- **Response**: Tr·∫°ng th√°i c·ªßa t·ª´ng service (Whisper, Pronunciation, LLM, Audio)
- **Chi ti·∫øt ki·ªÉm tra**:
  - Whisper model status v√† th√¥ng tin
  - Pronunciation service availability  
  - LLM service status (t·∫°m th·ªùi disabled)
  - Audio service availability

#### 1.3. `POST /evaluate-pronunciation` - Pipeline Ch√≠nh
```python
@app.post("/evaluate-pronunciation", response_model=PronunciationResponse)
async def evaluate_pronunciation(request: PronunciationRequest):
```

**LU·ªíNG X·ª¨ L√ù CHI TI·∫æT - 5 B∆Ø·ªöC:**

##### B∆∞·ªõc 1: Validation Input
```python
if not request.audio_base64:
    raise HTTPException(status_code=400, detail="Audio data is required")
if not request.sentence:
    raise HTTPException(status_code=400, detail="Reference sentence is required")
```
- Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o
- ƒê·∫£m b·∫£o c√≥ audio v√† c√¢u tham chi·∫øu

##### B∆∞·ªõc 2: Audio Quality Analysis
```python
audio_analysis = audio_service.analyze_audio_base64(request.audio_base64)
```
- **M·ª•c ƒë√≠ch**: Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng audio tr∆∞·ªõc khi x·ª≠ l√Ω
- **Service**: `AudioService`
- **Ki·ªÉm tra**: 
  - ƒê·ªô d√†i audio (0.5s - 30s)
  - Sample rate (‚â•8kHz, t·ªëi ∆∞u 16kHz)
  - M·ª©c √¢m l∆∞·ª£ng (tr√°nh qu√° nh·ªè/qu√° l·ªõn)
  - Noise level (SNR estimation)
  - Audio clipping detection
- **K·∫øt qu·∫£**: `AudioAnalysis` object v·ªõi quality score v√† danh s√°ch issues
- **X·ª≠ l√Ω l·ªói**: Reject n·∫øu audio kh√¥ng ƒë·∫°t ch·∫•t l∆∞·ª£ng t·ªëi thi·ªÉu

##### B∆∞·ªõc 3: Speech-to-Text Transcription
```python
transcribed_text, confidence = whisper_service.transcribe_audio_base64(
    request.audio_base64
)
```
- **M·ª•c ƒë√≠ch**: Chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i th√†nh vƒÉn b·∫£n
- **Service**: `WhisperService` (model: "small")
- **X·ª≠ l√Ω**:
  - Audio preprocessing (mono conversion, normalization)
  - Audio enhancement (boost quiet audio, remove DC offset)
  - Whisper transcription v·ªõi settings t·ªëi ∆∞u
  - Confidence calculation t·ª´ segments
- **K·∫øt qu·∫£**: Transcribed text + confidence score (0-1)
- **X·ª≠ l√Ω l·ªói**: Reject n·∫øu transcription th·∫•t b·∫°i

##### B∆∞·ªõc 4: Pronunciation Evaluation  
```python
scores, word_errors, wer_score = pronunciation_service.evaluate_pronunciation(
    request.sentence, transcribed_text, confidence
)
```
- **M·ª•c ƒë√≠ch**: So s√°nh ph√°t √¢m v·ªõi c√¢u tham chi·∫øu
- **Service**: `PronunciationService`
- **Thu·∫≠t to√°n**:
  - Phonetic conversion (text ‚Üí IPA-like phonetics)
  - Word Error Rate (WER) calculation
  - Word-level error detection (substitution/insertion/deletion)
  - Scoring calculation (pronunciation/fluency/intonation/stress)
- **K·∫øt qu·∫£**: Scores, word errors list, WER score

##### B∆∞·ªõc 5: Error Highlighting & Feedback Generation
```python
highlighted_sentence = pronunciation_service.highlight_errors(request.sentence, word_errors)
feedback = pronunciation_service.get_feedback(scores, word_errors)

# Enhanced LLM feedback (n·∫øu c√≥ API key)
llm_feedback = llm_service.generate_pronunciation_feedback(
    request.sentence, transcribed_text, scores, word_errors, wer_score
)
```
- **Error Highlighting**: T·∫°o c√¢u v·ªõi l·ªói ƒë∆∞·ª£c ƒë√°nh d·∫•u
- **Built-in Feedback**: Feedback c∆° b·∫£n d·ª±a tr√™n scores v√† errors
- **LLM Enhancement**: Feedback chi ti·∫øt t·ª´ Gemini AI (optional)

#### 1.4. `POST /debug-pronunciation` - Debug Pipeline
```python
@app.post("/debug-pronunciation")
async def debug_pronunciation(request: PronunciationRequest):
```
- **Ch·ª©c nƒÉng**: Endpoint debug v·ªõi th√¥ng tin chi ti·∫øt t·ª´ng b∆∞·ªõc
- **S·ª≠ d·ª•ng**: Development v√† troubleshooting
- **Response**: JSON v·ªõi chi ti·∫øt t·ª´ng b∆∞·ªõc x·ª≠ l√Ω

---

## 2. SERVICES - Microservices Architecture

### 2.1. WhisperService (`services/whisper_service.py`)

#### Ch·ª©c NƒÉng
Speech-to-Text service s·ª≠ d·ª•ng OpenAI Whisper model.

#### Methods Chi Ti·∫øt

##### `__init__(model_size="tiny")`
```python
def __init__(self, model_size: str = "tiny"):
    self.model = whisper.load_model(model_size)
```
- Load Whisper model (tiny/small/medium/large)
- Main.py s·ª≠ d·ª•ng "small" cho ƒë·ªô ch√≠nh x√°c t·ªët h∆°n

##### `transcribe_audio_base64(audio_base64)`
**LU·ªíNG X·ª¨ L√ù:**

1. **Audio Decoding**:
   ```python
   audio_bytes = base64.b64decode(audio_base64)
   audio_data, sample_rate = sf.read(io.BytesIO(audio_bytes))
   ```

2. **Audio Preprocessing**:
   ```python
   # Convert to mono
   if len(audio_data.shape) > 1:
       audio_data = np.mean(audio_data, axis=1)
   
   # Convert to float32
   audio_data = audio_data.astype(np.float32)
   ```

3. **Audio Enhancement** (`_enhance_audio()`):
   ```python
   # Remove DC offset
   audio_data = audio_data - np.mean(audio_data)
   
   # Boost quiet audio
   if rms < 0.05:
       boost_factor = 0.1 / (rms + 1e-8)
       audio_data = audio_data * boost_factor
   
   # Normalize v·ªõi headroom
   audio_data = audio_data / max_val * 0.95
   ```

4. **Whisper Transcription**:
   ```python
   result = self.model.transcribe(
       audio_data,
       language="en",
       task="transcribe", 
       fp16=False,
       condition_on_previous_text=False,
       temperature=0.0,
       compression_ratio_threshold=2.4,
       logprob_threshold=-1.0,
       no_speech_threshold=0.6
   )
   ```

5. **Confidence Calculation** (`_calculate_confidence()`):
   - S·ª≠ d·ª•ng word-level probabilities t·ª´ segments
   - Convert avg_logprob th√†nh confidence score
   - Clamp gi·ªØa 0.1-1.0

### 2.2. AudioService (`services/audio_service.py`)

#### Ch·ª©c NƒÉng
Audio quality analysis v√† validation service.

#### Methods Chi Ti·∫øt

##### `analyze_audio_base64(audio_base64)`
**LU·ªíNG PH√ÇN T√çCH:**

1. **Audio Loading**:
   ```python
   audio_bytes = base64.b64decode(audio_base64)
   audio_data, sample_rate = sf.read(io.BytesIO(audio_bytes))
   ```

2. **Basic Info Extraction**:
   - Duration calculation: `len(audio_data) / sample_rate`
   - Channel detection v√† mono conversion
   - Sample rate validation

3. **Quality Analysis** (`_analyze_quality()`):

   **Duration Check**:
   ```python
   if duration < self.min_duration:  # 0.5s
       issues.append(f"Audio too short ({duration:.1f}s)")
   elif duration > self.max_duration:  # 30s  
       issues.append(f"Audio very long ({duration:.1f}s)")
   ```

   **Sample Rate Check**:
   ```python
   if sample_rate < 8000:
       issues.append(f"Low sample rate ({sample_rate}Hz)")
   elif sample_rate < 16000:
       issues.append(f"Sample rate could be higher")
   ```

   **Volume Level Check**:
   ```python
   rms_level = np.sqrt(np.mean(audio_data**2))
   if rms_level < 0.01:
       issues.append("Audio level very low")
   elif rms_level < 0.05:
       issues.append("Audio level low")
   ```

   **Clipping Detection**:
   ```python
   clipping_ratio = np.sum(np.abs(audio_data) > 0.95) / len(audio_data)
   if clipping_ratio > 0.01:
       issues.append("Audio clipping detected")
   ```

   **SNR Estimation** (`_estimate_snr()`):
   ```python
   # Frame-based energy analysis
   energies = []
   for i in range(0, len(audio) - frame_size, hop_size):
       frame = audio[i:i + frame_size]
       energy = np.sum(frame**2)
       energies.append(energy)
   
   # Noise vs Signal energy
   noise_energy = np.percentile(energies, 10)    # 10th percentile
   signal_energy = np.percentile(energies, 90)   # 90th percentile
   snr_db = 10 * np.log10(signal_energy / noise_energy)
   ```

4. **Quality Score Calculation**:
   ```python
   quality_score = np.mean(quality_factors) * 100
   ```

5. **Validation Decision**:
   ```python
   critical_issues = [issue for issue in issues if "warning" not in issue.lower()]
   is_valid = len(critical_issues) == 0
   ```

### 2.3. PronunciationService (`services/pronunciation_service.py`)

#### Ch·ª©c NƒÉng
Phonetic-based pronunciation evaluation service.

#### Phonetic Mapping System
```python
self.phonetic_map = {
    # Vowels
    'a': '√¶', 'e': '…õ', 'i': '…™', 'o': '…î', 'u': ' ä',
    'ai': 'a…™', 'ay': 'e…™', 'oo': 'u', 'ou': 'a ä', 'ow': 'a ä',
    
    # Consonants  
    'th': 'Œ∏', 'sh': ' É', 'ch': 't É', 'ph': 'f', 'gh': 'f',
    'ck': 'k', 'qu': 'kw', 'x': 'ks', 'ng': '≈ã'
}
```

#### Methods Chi Ti·∫øt

##### `evaluate_pronunciation(original_text, transcribed_text, confidence)`
**LU·ªíNG ƒê√ÅNH GI√Å:**

1. **Phonetic Conversion**:
   ```python
   original_phonetic = self.text_to_phonetic(original_text)
   transcribed_phonetic = self.text_to_phonetic(transcribed_text)
   ```

2. **WER Calculation**:
   ```python
   wer_score = self.calculate_wer(original_phonetic, transcribed_phonetic)
   ```

3. **Word-level Error Detection**:
   ```python
   word_errors = self.get_word_errors(original_text, transcribed_text)
   ```

4. **Score Calculation**:
   ```python
   scores = self.calculate_scores(wer_score, confidence, len(word_errors))
   ```

##### `text_to_phonetic(text)`
```python
def text_to_phonetic(self, text: str) -> str:
    text = text.lower().strip()
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Apply phonetic mappings (longest first)
    for pattern, phonetic in sorted(self.phonetic_map.items(), key=len, reverse=True):
        text = text.replace(pattern, phonetic)
    
    return ' '.join(text.split())
```

##### `get_word_errors(original, transcribed)`
**S·ª≠ d·ª•ng difflib.SequenceMatcher**:
```python
matcher = difflib.SequenceMatcher(None, orig_words, trans_words)

for tag, i1, i2, j1, j2 in matcher.get_opcodes():
    if tag == 'replace':  # Substitution
        # Calculate phonetic similarity
        severity = self.get_phonetic_similarity(orig_word, trans_word)
        
    elif tag == 'delete':  # Deletion (missing word)
        # Mark as missing word
        
    elif tag == 'insert':  # Insertion (extra word)  
        # Mark as extra word
```

##### `calculate_scores(wer_score, confidence, error_count)`
```python
def calculate_scores(self, wer_score: float, confidence: float, error_count: int):
    pronunciation = max(0, (1 - wer_score) * 100)
    fluency = min(100, confidence * 85 + (1 - wer_score) * 15)
    intonation = min(100, confidence * 70 + 30)  
    stress = max(30, pronunciation * 0.8 + confidence * 20)
    overall = pronunciation * 0.5 + fluency * 0.3 + intonation * 0.1 + stress * 0.1
```

##### `highlight_errors(original_text, word_errors)`
```python
def highlight_errors(self, original_text: str, word_errors: List[WordError]) -> str:
    for error in sorted(word_errors, key=lambda x: x.position, reverse=True):
        if error.error_type == "substitution":
            highlighted[error.position] = f"[‚ùå{error.expected}‚Üí{error.actual}]"
        elif error.error_type == "deletion":
            highlighted[error.position] = f"[‚ùåTHI·∫æU:{error.expected}]"
        elif error.error_type == "insertion":
            highlighted.append(f"[‚ùåTH√äM:{error.actual}]")
```

### 2.4. LLMService (`services/llm_service.py`)

#### Ch·ª©c NƒÉng
AI-powered feedback generation s·ª≠ d·ª•ng Google Gemini API.

#### Methods Chi Ti·∫øt

##### `__init__()`
```python
def __init__(self):
    self.gemini_api_key = os.getenv("GEMINI_API_KEY")
```
- Load API key t·ª´ `.env.new` file
- Graceful fallback n·∫øu kh√¥ng c√≥ API key

##### `generate_pronunciation_feedback(...)`
**LU·ªíNG T·∫†O FEEDBACK:**

1. **Prompt Construction**:
   ```python
   error_summary = self._format_errors(word_errors)
   
   prompt = f"""As an English pronunciation teacher for Vietnamese learners:
   
   Original: "{original_sentence}"
   Student: "{transcribed_text}"
   Scores: Overall {scores.overall}/100, Pronunciation {scores.pronunciation}/100...
   Errors: {error_summary}
   
   Provide structured feedback with:
   üåü Introduction with encouragement
   üìä Error Analysis  
   ‚úÖ Corrective Actions
   üìö Additional Resources
   üí™ Words of Encouragement
   """
   ```

2. **Gemini API Call**:
   ```python
   url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={self.gemini_api_key}"
   
   data = {
       "contents": [{"parts": [{"text": prompt}]}],
       "generationConfig": {
           "temperature": 0.7,
           "maxOutputTokens": 400,
           "candidateCount": 1
       }
   }
   ```

3. **Response Processing**:
   ```python
   if response.status_code == 200:
       result = response.json()
       return result["candidates"][0]["content"]["parts"][0]["text"].strip()
   ```

##### `_format_errors(word_errors)`
```python
def _format_errors(self, word_errors: List[WordError]) -> str:
    errors = []
    for error in word_errors[:3]:  # Top 3 errors
        if error.error_type == "substitution":
            errors.append(f"'{error.expected}' ‚Üí '{error.actual}'")
        elif error.error_type == "deletion":
            errors.append(f"Missing '{error.expected}'")
        elif error.error_type == "insertion":
            errors.append(f"Extra '{error.actual}'")
    
    return "; ".join(errors)
```

---

## 3. MODELS.PY - Data Models

### 3.1. Core Models

#### `PronunciationScore`
```python
class PronunciationScore(BaseModel):
    pronunciation: float  # 0-100 - ƒê·ªô ch√≠nh x√°c ph√°t √¢m
    fluency: float       # 0-100 - ƒê·ªô tr√¥i ch·∫£y
    intonation: float    # 0-100 - Ng·ªØ ƒëi·ªáu  
    stress: float        # 0-100 - Tr·ªçng √¢m
    overall: float       # 0-100 - ƒêi·ªÉm t·ªïng th·ªÉ
```

#### `WordError`
```python
class WordError(BaseModel):
    word: str           # T·ª´ b·ªã l·ªói
    position: int       # V·ªã tr√≠ trong c√¢u (0-based)
    error_type: str     # substitution/insertion/deletion
    expected: str       # T·ª´ mong ƒë·ª£i
    actual: str         # T·ª´ th·ª±c t·∫ø
    severity: str       # low/moderate/high
```

#### `AudioAnalysis`
```python
class AudioAnalysis(BaseModel):
    is_valid: bool         # Audio c√≥ h·ª£p l·ªá kh√¥ng
    duration: float        # ƒê·ªô d√†i (gi√¢y)
    sample_rate: int       # T·∫ßn s·ªë l·∫•y m·∫´u (Hz)
    channels: int          # S·ªë k√™nh √¢m thanh
    issues: List[str]      # Danh s√°ch v·∫•n ƒë·ªÅ
    quality_score: float   # ƒêi·ªÉm ch·∫•t l∆∞·ª£ng (0-100)
```

### 3.2. Request/Response Models

#### `PronunciationRequest`
```python
class PronunciationRequest(BaseModel):
    audio_base64: str    # Audio data encoded base64
    sentence: str        # C√¢u tham chi·∫øu ƒë·ªÉ so s√°nh
```

#### `PronunciationResponse` 
```python
class PronunciationResponse(BaseModel):
    original_sentence: str        # C√¢u g·ªëc
    transcribed_text: str        # K·∫øt qu·∫£ transcription
    scores: PronunciationScore   # ƒêi·ªÉm s·ªë chi ti·∫øt
    word_errors: List[WordError] # Danh s√°ch l·ªói t·ª´ng t·ª´
    feedback: str               # Ph·∫£n h·ªìi t·ª´ AI
    wer_score: float           # Word Error Rate
    confidence: float          # ƒê·ªô tin c·∫≠y c·ªßa transcription
    highlighted_sentence: str  # C√¢u v·ªõi l·ªói ƒë∆∞·ª£c highlight
```

---

## 4. LU·ªíNG HO·∫†T ƒê·ªòNG T·ªîNG TH·ªÇ

### 4.1. End-to-End Workflow

```
[Client] ‚îÄ‚îÄ(audio + sentence)‚îÄ‚îÄ‚ñ∂ [FastAPI Gateway]
                                        ‚îÇ
                                        ‚ñº
                                [Input Validation]
                                        ‚îÇ
                                        ‚ñº
                               [AudioService Analysis]
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ quality_score ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ                          ‚ñº
                            ‚îÇ                    [REJECT if bad]
                            ‚ñº
                    [WhisperService Transcription]
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ transcribed_text ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ        confidence              ‚îÇ
                    ‚ñº                                ‚ñº
          [PronunciationService Evaluation]         ‚îÇ
          ‚îÇ                                         ‚îÇ
          ‚îú‚îÄ Phonetic Conversion                    ‚îÇ
          ‚îú‚îÄ WER Calculation                        ‚îÇ
          ‚îú‚îÄ Word Error Detection                   ‚îÇ
          ‚îú‚îÄ Score Calculation                      ‚îÇ
          ‚îî‚îÄ Error Highlighting                     ‚îÇ
                    ‚îÇ                                ‚îÇ
                    ‚ñº                                ‚ñº
           [Built-in Feedback] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [LLMService Enhancement]
                    ‚îÇ                                ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚ñº
                         [Final Response]
                                 ‚îÇ
                                 ‚ñº
                            [Client]
```

### 4.2. Error Handling Strategy

#### Level 1: Input Validation
- Missing audio/sentence ‚Üí HTTP 400
- Invalid base64 encoding ‚Üí HTTP 400

#### Level 2: Audio Quality Check  
- Duration too short/long ‚Üí HTTP 400
- Low quality audio ‚Üí HTTP 400  
- No speech detected ‚Üí HTTP 400

#### Level 3: Service Failures
- Whisper transcription fails ‚Üí HTTP 400
- Empty transcription ‚Üí HTTP 400
- Service exceptions ‚Üí HTTP 500

#### Level 4: Graceful Fallbacks
- LLM service unavailable ‚Üí Use built-in feedback
- Pronunciation service errors ‚Üí Return partial results
- Audio enhancement fails ‚Üí Continue with original audio

### 4.3. Performance Optimizations

#### Audio Processing
- Audio enhancement for low-quality recordings
- Optimal Whisper model size (small = accuracy vs speed balance)
- Efficient audio format handling

#### Pronunciation Evaluation
- Simplified phonetic mapping for speed
- Optimized WER calculation
- Selective error highlighting  

#### LLM Integration
- Timeout handling (30s)
- Structured prompts for consistent output
- Error formatting for concise feedback
- Optional enhancement (graceful fallback)

---

## 5. CONFIGURATION & DEPLOYMENT

### 5.1. Environment Setup
```bash
# Required packages
whisper-openai>=20230124
fastapi>=0.68.0
uvicorn>=0.15.0
jiwer>=2.3.0
soundfile>=0.10.0
numpy>=1.21.0
requests>=2.25.0
```

### 5.2. Optional API Keys
```bash
# .env.new file
GEMINI_API_KEY=your_gemini_api_key_here
```

### 5.3. Model Configuration
- **Whisper Model**: "small" (balance accuracy/speed)
- **Audio Requirements**: 0.5-30s, ‚â•8kHz, mono preferred
- **Language**: English only
- **Output Format**: JSON responses

### 5.4. Service Endpoints
```
POST /evaluate-pronunciation - Main evaluation pipeline
POST /debug-pronunciation   - Debug v·ªõi detailed info  
GET  /health                - Service health check
GET  /                     - Welcome message
```

---

## 6. TROUBLESHOOTING

### Common Issues

#### Audio Problems
- **"Audio too short"**: Minimum 0.5 seconds required
- **"Audio level very low"**: Speak closer to microphone  
- **"High background noise"**: Find quieter environment
- **"Audio clipping"**: Reduce microphone gain

#### Transcription Issues
- **Empty transcription**: Check audio quality and clarity
- **Low confidence**: Improve audio quality or speak clearer
- **Wrong language**: System only supports English

#### Service Errors
- **LLM unavailable**: Check API key and internet connection
- **Whisper model loading**: Ensure sufficient disk space
- **Memory issues**: Consider smaller Whisper model

### Debug Tools
- Use `/debug-pronunciation` endpoint for step-by-step analysis
- Check `/health` endpoint for service status
- Monitor console logs for detailed error information

---

## K·∫øt Lu·∫≠n

H·ªá th·ªëng AI English Learning Server s·ª≠ d·ª•ng ki·∫øn tr√∫c microservices v·ªõi lu·ªìng x·ª≠ l√Ω ƒëa t·∫ßng, t·ª´ validation ƒë·∫ßu v√†o ƒë·∫øn AI feedback generation. M·ªói service c√≥ vai tr√≤ ri√™ng bi·ªát nh∆∞ng ph·ªëi h·ª£p ch·∫∑t ch·∫Ω ƒë·ªÉ t·∫°o ra tr·∫£i nghi·ªám h·ªçc ph√°t √¢m ho√†n ch·ªânh v√† ch√≠nh x√°c.

**ƒêi·ªÉm m·∫°nh ch√≠nh**:
- Pipeline x·ª≠ l√Ω robust v·ªõi multiple validation layers
- Graceful fallback cho service failures  
- Detailed error analysis v√† feedback
- Scalable architecture v·ªõi clear separation of concerns
- Support cho multiple audio formats v√† quality levels
